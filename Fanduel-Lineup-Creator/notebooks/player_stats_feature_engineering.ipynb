{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to produce more relevant variables which can be used to improve each NBA player model. This requires connecting to the AWS Redshift database created in the <a href='https://github.com/Eric-Pacheco95/PersonalProjects/blob/master/Fanduel-Lineup-Creator/notebooks/player_stats.ipynb'>player_stats notebook</a> and then after feature engineering and formatting, the new data was uploaded and saved to a new AWS Redshift database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from basketball_reference_web_scraper import client\n",
    "import pandas as pd\n",
    "import datetime as datetime\n",
    "import joblib\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the player_positions, team_label_encoder, player_label_encoder saved in the player_stats notebook. After loading, create a list containing all player slugs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "player_positions = joblib.load('../joblib_objects/player_positions')\n",
    "team_label_encoder = joblib.load('../joblib_objects/team_label_encoder')\n",
    "player_label_encoder = joblib.load('../joblib_objects/player_label_encoder')\n",
    "\n",
    "player_slugs = list(player_positions.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect to AWS Redshift database created in the player_stats notebook and create an engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import *\n",
    "from sqlalchemy.orm import Session\n",
    "from sqlalchemy import Table, Column, String, MetaData, Integer, Float\n",
    "\n",
    "user = os.environ['RDS_NBA_DATABASE_USER']\n",
    "password = os.environ['RDS_NBA_DATABASE_PASSWORD']\n",
    "\n",
    "db_scraped_data = create_engine(f'postgresql://{user}:{password}@fanduel-lineup-prediction-scraped-data.cvzkizpca2fx.us-east-1.rds.amazonaws.com')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the master advanced analytics csv file, and save the column names as a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Simple_Rating_System', 'Offensive_Rating', 'Defensive_Rating',\n",
       "       'Net_Rating', 'Pace', 'Free_Throw_Rate', '3_Pt_Rate',\n",
       "       'Turnover_Percentage', 'Offensive_Rebound_Percentage', 'Opponent_EFG',\n",
       "       'Opponent_Turnover_Percentage', 'Opponent_Defensive_Rebound_Percentage',\n",
       "       'Team_ID', 'year'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "advanced_analytics_table = pd.read_csv('../cleaned_data/advanced_analytics/advanced_analytics_total.csv',index_col=0)\n",
    "advanced_analytics_columns = advanced_analytics_table.columns \n",
    "advanced_analytics_columns[2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create multiple lists which will hold every date during each respective NBA season."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nba_season_start_end_dates = {'2016-2017':[datetime.datetime(2016,10,25),datetime.datetime(2017,4,12)],\n",
    "                             '2017-2018':[datetime.datetime(2017,10,17),datetime.datetime(2018,4,11)],\n",
    "                             '2018-2019':[datetime.datetime(2018,10,16),datetime.datetime(2019,4,10)],\n",
    "                             '2019-2020':[datetime.datetime(2019,10,22),datetime.datetime(2020,8,14)]}\n",
    "\n",
    "nba_2016_to_2017_dates = [nba_season_start_end_dates['2016-2017'][0] + datetime.timedelta(days=x) for x in range((nba_season_start_end_dates['2016-2017'][1]-nba_season_start_end_dates['2016-2017'][0]).days+1)]\n",
    "nba_2017_to_2018_dates = [nba_season_start_end_dates['2017-2018'][0] + datetime.timedelta(days=x) for x in range((nba_season_start_end_dates['2017-2018'][1]-nba_season_start_end_dates['2017-2018'][0]).days+1)]\n",
    "nba_2018_to_2019_dates = [nba_season_start_end_dates['2018-2019'][0] + datetime.timedelta(days=x) for x in range((nba_season_start_end_dates['2018-2019'][1]-nba_season_start_end_dates['2018-2019'][0]).days+1)]\n",
    "nba_2019_to_2020_dates = [nba_season_start_end_dates['2019-2020'][0] + datetime.timedelta(days=x) for x in range((nba_season_start_end_dates['2019-2020'][1]-nba_season_start_end_dates['2019-2020'][0]).days+1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function which will determine which NBA season a data falls under. This is required to determine which set of advanced analytics to attribute to game data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_season_year(x):\n",
    "    if x in nba_2016_to_2017_dates:\n",
    "        return 2016\n",
    "    elif x in nba_2017_to_2018_dates:\n",
    "        return 2017\n",
    "    elif x in nba_2018_to_2019_dates:\n",
    "        return 2018\n",
    "    else:\n",
    "        return 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function which will retrieve the respective advanced analytics by filtering for season and opponent_id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_advanced_analytics(x):\n",
    "    analytics = advanced_analytics_table.loc[(x['season'] == advanced_analytics_table['year']) & (x['opponent_id'] == advanced_analytics_table['Team_ID']),advanced_analytics_columns[2:]]\n",
    "    \n",
    "    return analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect to new AWS Redshift database which will be used to hold new data after feature engineering and formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_cleaned_data = create_engine(f'postgresql://{user}:{password}@fanduel-lineup-prediction-cleaned-data.cvzkizpca2fx.us-east-1.rds.amazonaws.com')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop over player_slugs and add player rest features as columns, add respective advanced analytics as columns, and finally save the newly created data as a table in the new AWS Redshift database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Eric\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:2702: UserWarning: the 'timedelta' type is not supported, and will be written as integer values (ns frequency) to the database.\n",
      "  sql.to_sql(\n"
     ]
    }
   ],
   "source": [
    "for player in player_slugs:\n",
    "    if player not in db_cleaned_data.table_names():\n",
    "        df = pd.read_sql_table(player,db_scraped_data,index_col='index')\n",
    "\n",
    "        df['rest'] = df['date'] - df['date'].shift(1) - datetime.timedelta(days=1)\n",
    "\n",
    "        df['no_rest'] = df['rest'].apply(lambda x: 1 if (x == datetime.timedelta(days=0)) else 0)\n",
    "        df['1_day_rest'] = df['rest'].apply(lambda x: 1 if (x == datetime.timedelta(days=1)) else 0)\n",
    "        df['2_day_rest'] = df['rest'].apply(lambda x: 1 if (x == datetime.timedelta(days=2)) else 0)\n",
    "        df['3_day_rest'] = df['rest'].apply(lambda x: 1 if (x == datetime.timedelta(days=3)) else 0)\n",
    "        df['4_day_rest'] = df['rest'].apply(lambda x: 1 if (x == datetime.timedelta(days=4)) else 0)\n",
    "        df['5_day_rest'] = df['rest'].apply(lambda x: 1 if (x == datetime.timedelta(days=5)) else 0)\n",
    "        df['5_plus_day_rest'] = df['rest'].apply(lambda x: 1 if (x > datetime.timedelta(days=5)) else 0)\n",
    "\n",
    "\n",
    "        df['season'] = df['date'].apply(get_season_year)\n",
    "        original_columns = df.columns\n",
    "\n",
    "        for column in advanced_analytics_columns[2:]:\n",
    "            df[column] = ''\n",
    "\n",
    "        for row in range(len(df)):\n",
    "            analytics = get_advanced_analytics(df.iloc[row,:]).values[0]\n",
    "            original_stats = df.iloc[row,:].to_list()\n",
    "            original_stats[33:] = analytics\n",
    "\n",
    "            df.iloc[row,:] = original_stats\n",
    "\n",
    "        df.to_sql(player, db_cleaned_data, if_exists='append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
